#+OPTIONS: num:nil toc:nil
file:branding/logo/bless_logo.png
#+TOC: headlines 2
* About
A simple command line wrapper for repeated runs, with metadata and lightweight
tracking.
** Why?
During development, a full interface to HPC oriented workflow engines like
AiiDA, Fireworks, Jobflow, and the like is typically too heavy, and more
importantly, the API is often not stable. That being said, this could also be
used in conjunction with ~pychum~ and workflow runners like Snakemake to store
full logs for later.
** Features
- Fast, compressed, logging, with almost zero overhead
- File and MongoDB interface
  + Including large log support via GridFS
  + Helper script also provided to recreate logs
** Design
*** File (gzip) Writer
This overrides the ~Log~ levels of Rust, so:
- ~TRACE~ is for additional information for the command, as written by ~bless~
- ~INFO~ corresponds to ~stdout~ of the command
- ~ERROR~ corresponds to a ~bless~ error
- ~WARN~ corresponds to ~stderr~ of the command
*** MongoDB Writer
Only ~stderr~ and ~stdout~ of the command are stored in a ~.gz~ file which is
added to the database as a binary blob, with additional metadata.
* Usage
#+begin_src bash
cargo build --release
MONGODB_URI="mongodb://localhost:27017/" ./target/release/bless --use-mongodb -- echo "bye"
# Then view it in mongosh
# or
./target/release/bless -- echo "bye"
zcat default_label*.gz
#+end_src
* Development
** Component Rationale
- Duct :: For [[https://github.com/oconnor663/duct.py/blob/master/gotchas.md][the gotchas]]
- Wild :: For cross-platform globs
- Flate2 :: For compression
- UUID ::  For the unique IDs
- Fern :: For log handling
** Local MongoDB
Assuming ~pixi~ is used to get an instance of ~mongod~.
#+begin_src bash
pixi run mongod --dbpath $(pwd)/data/database
MONGODB_URI="mongodb://localhost:27017/" ./target/release/bless --use-mongodb -- $CMD_TO_RUN
#+end_src
I use ~npx mongosh~ for validating commands.
#+begin_src bash
npx mongsh
use local
# Show all entries
db.commands.find()
# Suppress blob data
db.commands.find({}, { gzip_blob: 0 })
# Dangerous, drop all entries!
db.getCollectionNames().forEach(c=>db[c].drop())
#+end_src
*** Extracting run output
Since the ~.gzip~ is stored as binary data keyed to the ~gzip_blob~ entry, a
small helper script is provided.
#+begin_src python
python scripts/get_db_gzip.py --db-name local --collection-name commands --query-field label --query-value np_nogrid --output-file output.gzip
#+end_src
We can check that the generated file is the same.
#+begin_src bash
sha256sum output.gzip np_nogrid_b7b3a733-7383-4367-b5c6-abaf55051114.log.gz
71b3678846c29daa1c486473f7770b72ff08b99001d26e38df81662e6eeedc3f  output.gzip
71b3678846c29daa1c486473f7770b72ff08b99001d26e38df81662e6eeedc3f  np_nogrid_b7b3a733-7383-4367-b5c6-abaf55051114.log.gz
# or
diff output.gzip np_nogrid*.gz
#+end_src
*** Large File Support
When a log is larger than 15 MB, ~bless~ will use GridFS to chunk and store the
file. The script automatically handles the case of having large log files by
using GridFS to assemble them from ~gzip_blob_id~. This can be requested via the
command-line manually as well.
** Documentation
We use ~cocogitto~ via ~cog~ to handle commit conventions.
*** Readme
The ~readme~ can be constructed via:
#+begin_src bash
./scripts/org_to_md.sh readme_src.org readme.md
#+end_src
metadata more generically.
* License
MIT. However, this is an academic resource, so *please cite* as much as possible
via:
- The Zenodo DOI for general use.
- TBD a publication
